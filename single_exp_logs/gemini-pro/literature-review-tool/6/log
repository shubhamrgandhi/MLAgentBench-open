(MaxRetryError("HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Max retries exceeded with url: /repos/34/b1/34b149a3c1914f8d613d1b121f4484e4263cfe0d69bc2da1299ac07cad6e2eeb/9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tokenizer.model%3B+filename%3D%22tokenizer.model%22%3B&Expires=1707428547&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNzQyODU0N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9iMS8zNGIxNDlhM2MxOTE0ZjhkNjEzZDFiMTIxZjQ0ODRlNDI2M2NmZTBkNjliYzJkYTEyOTlhYzA3Y2FkNmUyZWViLzllNTU2YWZkNDQyMTNiNmJkMWJlMmI4NTBlYmJiZDk4ZjU0ODE0MzdhODAyMWFmYWY1OGVlN2ZiMTgxOGQzNDc~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=CYK0vmC3zHAbTwcuBFyZZ9MsbNFUEiy5su41ihcixzxWOwObSuHqUWi4mkrWS9MqEqAF6o6HYHJRDBSvxOmH7tujBODIbuQOq6W5ydDZoWZUbMIUsVU0rXMldmcjuwQvn~AqHTy7Jj-ulFm4fWt2B~Q38V5~bHXCue06PdtoegWwvpEHOfsrQIHTXAxDYIQla59s0EoJu4J-t9Zj50VPFoC7bTtVhU7FGV7zMBfrd5oS1t6hmPiRBnUzPkYEVL0jqkLhl8Kh-1sSY53wQqQrph5~oIfT9g2bnR8xkpasH7QMLexJ5PhsWAtP4ZkLQ4vstzRvuCPY9hKMPPSLaKo9YQ__&Key-Pair-Id=KVTP0A1DKRTAX (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))"), '(Request ID: 58097ff5-c25a-4d4c-8161-2362ae9c43f9)')
Unable to instantiate codellama huggingface api for inference. Check if API key is stored in file huggingface_api_key.txt. Ignore if not using codellama as one of the LLMs
Namespace(task='literature-review-tool', log_dir='single_exp_logs/gemini-pro/literature-review-tool/6', work_dir='workspaces/single_exp_logs/gemini-pro/literature-review-tool/6', max_steps=50, max_time=18000, device=0, python='python', interactive=False, resume=None, resume_step=0, agent_type='ResearchAgent', llm_name='gemini-pro', fast_llm_name='gemini-pro', edit_script_llm_name='gemini-pro', edit_script_llm_max_tokens=4000, agent_max_steps=30, actions_remove_from_prompt=[], actions_add_to_prompt=[], no_retrieval=False, valid_format_entires=None, max_steps_in_context=3, max_observation_steps_in_context=3, max_retries=5, langchain_agent='zero-shot-react-description')
log_dir single_exp_logs/gemini-pro/literature-review-tool/6\env_log already exists
C:\Users\2671148\MLAgentBench-open\MLAgentBench\benchmarks\literature-review-tool\scripts\prepare.py not found or already prepared
=====================================
Benchmark folder name:  literature-review-tool
Research problem:  Create a literature_review.py file to create a large language model(LLM) based AI system for the task of literature review. The LLM can be accessed using anthropic API with API key in claude_api_key.txt. Use it as in claude_example.py.
Given ANY query on a research topic, e.g. Language model hallucination detection, the system should 
    - Call arxiv API (with reference in arxiv_API_reference.txt) with appropriate queries to get a list of related papers.
    - Use the LLM to summarize these papers and generate a report. The report should include the following information: 1) the list of papers; 2) the summary of each paper; 3) the conclusion of the literature review.
Lower level actions enabled:  ['List Files', 'Read File', 'Write File', 'Append File', 'Copy File', 'Undo Edit Script', 'Execute Script', 'Python REPL', 'Final Answer']
High level actions enabled:  ['Understand File', 'Append Summary to Research Log', 'Inspect Script Lines', 'Edit Script (AI)', 'Edit Script Segment (AI)', 'Reflection', 'Retrieval from Research Log']
Read only files:  []
=====================================
Agent is up! See progress in single_exp_logs/gemini-pro/literature-review-tool/6\agent_log\main_log
Active Children: 0
Active Children: 0
Error message saved in error.txt
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "C:\Users\2671148\MLAgentBench-open\MLAgentBench\runner.py", line 79, in <module>
    run(getattr(sys.modules[__name__], args.agent_type), args)
  File "C:\Users\2671148\MLAgentBench-open\MLAgentBench\runner.py", line 31, in run
    final_message = agent.run(env)
                    ^^^^^^^^^^^^^^
  File "C:\Users\2671148\MLAgentBench-open\MLAgentBench\agents\agent_research.py", line 114, in run
    completion = complete_text(prompt, log_file, self.args.llm_name)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\2671148\MLAgentBench-open\MLAgentBench\LLM.py", line 266, in complete_text
    completion = complete_text_geminipro(prompt, log_file=log_file, model=model, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\2671148\MLAgentBench-open\MLAgentBench\LLM.py", line 229, in complete_text_geminipro
    log_to_file(log_file, prompt, completion, model, max_tokens_to_sample)
  File "C:\Users\2671148\MLAgentBench-open\MLAgentBench\LLM.py", line 76, in log_to_file
    if model == 'gemini-pro': num_sample_tokens = str(model_gemini.count_tokens(completion)).split(":")[-1].strip()
                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\2671148\AppData\Roaming\Python\Python311\site-packages\google\generativeai\generative_models.py", line 285, in count_tokens
    return self._client.count_tokens(glm.CountTokensRequest(model=self.model_name, contents=contents))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\2671148\AppData\Roaming\Python\Python311\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 1215, in count_tokens
    response = rpc(
               ^^^^
  File "C:\Users\2671148\AppData\Roaming\Python\Python311\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\2671148\AppData\Roaming\Python\Python311\site-packages\google\api_core\retry.py", line 372, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "C:\Users\2671148\AppData\Roaming\Python\Python311\site-packages\google\api_core\retry.py", line 207, in retry_target
    result = target()
             ^^^^^^^^
  File "C:\Users\2671148\AppData\Roaming\Python\Python311\site-packages\google\api_core\timeout.py", line 120, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\2671148\AppData\Roaming\Python\Python311\site-packages\google\api_core\grpc_helpers.py", line 79, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\2671148\AppData\Roaming\Python\Python311\site-packages\grpc\_channel.py", line 1157, in __call__
    ) = self._blocking(
        ^^^^^^^^^^^^^^^
  File "C:\Users\2671148\AppData\Roaming\Python\Python311\site-packages\grpc\_channel.py", line 1141, in _blocking
    event = call.next_event()
            ^^^^^^^^^^^^^^^^^
  File "src\python\grpcio\grpc\_cython\_cygrpc/channel.pyx.pxi", line 366, in grpc._cython.cygrpc.SegregatedCall.next_event
  File "src\python\grpcio\grpc\_cython\_cygrpc/channel.pyx.pxi", line 187, in grpc._cython.cygrpc._next_call_event
  File "src\python\grpcio\grpc\_cython\_cygrpc/channel.pyx.pxi", line 181, in grpc._cython.cygrpc._next_call_event
  File "src\python\grpcio\grpc\_cython\_cygrpc/completion_queue.pyx.pxi", line 78, in grpc._cython.cygrpc._latent_event
  File "src\python\grpcio\grpc\_cython\_cygrpc/completion_queue.pyx.pxi", line 61, in grpc._cython.cygrpc._internal_latent_event
  File "src\python\grpcio\grpc\_cython\_cygrpc/completion_queue.pyx.pxi", line 42, in grpc._cython.cygrpc._next
KeyboardInterrupt
^C